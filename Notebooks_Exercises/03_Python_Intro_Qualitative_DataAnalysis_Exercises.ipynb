{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b685818-8688-4178-b101-62657d1cbb11",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c854b69-3775-4adf-8040-2edd8f8402ef",
   "metadata": {},
   "source": [
    "# Data Analysis in Python\n",
    "\n",
    "This notebook provides an introduction to basic qualitative data analysis in Python, building on earlier notebooks in syntax and data handling. It demonstrates how to process and analyze text data using Python, including searching, filtering, and counting occurrences within text. Parts of this notebook were inspired by or adapted from the Python Humanities textbook, available at [python-textbook.pythonhumanities.com](python-textbook.pythonhumanities.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c079869-1329-4c60-b6c2-d4a83484451a",
   "metadata": {},
   "source": [
    "## 1. Plot Data with `pandas`\n",
    "\n",
    "`pandas` makes it easy to create quick and simple visualizations directly from a DataFrame using the `.plot()` method. This is especially useful for exploring your data before doing more complex analyses or creating polished plots with other libraries such as `matplotlib`. \n",
    "\n",
    "Here are three example plots created using `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3cfb46-6ab7-43ea-8ec4-24b8cddc4a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117aad5c-1129-45f5-aacf-10eaa6b19e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original Titanic dataset includes only 'male' and 'female' as gender categories.\n",
    "# For demonstration purposes, we'll change the first 3 entries to 'diverse'. This also shows how indexing and assignment work in pandas.\n",
    "Titanic = pd.read_csv(\"../Data/Titanic-Dataset.csv\")\n",
    "Titanic.loc[0:2, 'Sex'] = 'diverse'  # Change rows with index 0, 1, and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0be791-946e-4c52-b4e9-568e4e5a743d",
   "metadata": {},
   "source": [
    "1. A bar chart showing the number of male and female passengers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c01da-aea8-427f-916b-83b6277e81ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titanic['Sex'].value_counts().plot.bar(title=\"Title\") # Bar plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498c991-64f5-440d-ab4d-e48653fbf0e2",
   "metadata": {},
   "source": [
    "2. A pie chart showing the proportion of passengers in each travel class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed178ca-420a-4bf9-814a-5d732c5eb5da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Titanic['Pclass'].value_counts().plot.pie() # Pie plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e738f-2ca3-45dc-a8a3-8819b86eb9d4",
   "metadata": {},
   "source": [
    "3. A scatter plot to explore the relationship between the age and fare of passengers, colored by travel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4391bea-f984-4766-8306-df76beac4c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Titanic.plot.scatter(x=\"Age\", y=\"Fare\", c=\"Pclass\", cmap=\"viridis\") # Scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb6e9e1-6510-4374-98f2-e7e2b9952bbc",
   "metadata": {},
   "source": [
    "Now, let’s try this again by first converting Pclass into a `pandas` categorical type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c176acd-d39b-48ce-9ce8-be6def9bdbf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Titanic['Pclass'] = Titanic.Pclass.astype('category')\n",
    "Titanic.plot.scatter(x=\"Age\", y=\"Fare\", c=\"Pclass\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a6c0f-62a2-4c4e-96a1-04f6e162e68d",
   "metadata": {},
   "source": [
    "## 2. Manual Text Analysis and Pattern Matching with `re` (Regex)\n",
    "\n",
    "Before using complex tools, we often start with manual coding - spotting phrases, concepts, or features manually or with simple string rules. This can be automated using Python’s built-in regular expressions module, `re`.\n",
    "\n",
    "For example, **to extract dates:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89827f5-88ee-494b-84ee-3df080a3e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"The meeting happened on 12 March, 5 April, and 23 December.\"\n",
    "matches = re.findall(r\"\\b\\d{1,2} [A-Z][a-z]+\", text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684a60c-55b8-4bf1-80a7-74a133dcf8fe",
   "metadata": {},
   "source": [
    "- `\\b` word boundary, ensures we start at the beginning of a word\n",
    "- `\\d{1,2}` matches 1 or 2 digit day numbers\n",
    "- `[A-Z][a-z]+` matches any capitalized month name (or word), flexible for months"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a82e6a-e38a-487e-ad26-690dd8993deb",
   "metadata": {},
   "source": [
    "Limitation: It is not realistically possible to match all date formats with a single regular expression because dates can appear in a huge variety of formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac6f29-0133-4164-8b66-65e23efe9aca",
   "metadata": {},
   "source": [
    "### **Exercise 1:** \n",
    "\n",
    "What happens if you enter a typo to one of the month names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d31b50-4773-4b57-b20a-86250feada44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c20ce92a-b570-4f88-b0f2-836dd0d79b03",
   "metadata": {},
   "source": [
    "**To extract emails:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949e610-1b71-48fa-8205-7a900a743873",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Please contact us at info@example.com or support@service.org for assistance.\"\n",
    "emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba30d50-d5d4-4d72-bbe9-bb6baae76072",
   "metadata": {},
   "source": [
    "- `[...]` defines a character class\n",
    "- `a-zA-Z` allows uppercase and lowercase letters\n",
    "- `0-9` allows numbers\n",
    "- `._%+-` allows typical symbols used in emails\n",
    "- `+` means one or more of the preceding characters\n",
    "- `@` a literal match of the @ character\n",
    "- `\\.` dot before the top level domain (TLD)\n",
    "- `[a-zA-Z]{2,}` TLD allows only letters and `{2,}` means \"at least 2 characters\" (e.g., com, org, de, info, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac51705-a44d-4bca-b8de-ad801d75dac4",
   "metadata": {},
   "source": [
    "## 3. Very Brief Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing (NLP) helps us extract structure and meaning from unstructured text, like interviews, news articles, or historical records. It powers things like keyword tagging, topic detection, and identifying people or places."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c155e749-73f4-445f-814f-65203184d629",
   "metadata": {},
   "source": [
    "## 3.1 Tokenization and Word Frequency\n",
    "\n",
    "This is how we can **tokenize text** with basic Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67cabf-00d0-490e-ac80-f4aea1cb207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "text = \"I love learning Python. Python is great for data analysis and Python is also fun!\"\n",
    "tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b965f8f-9229-4bef-afbc-3d14bb800d26",
   "metadata": {},
   "source": [
    "- \\b\\w+\\b: match each word\n",
    "- text.lower() normalizes casing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da60bc05-e0cd-4c45-81b6-23ef1e786c41",
   "metadata": {},
   "source": [
    "We can also **count top words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2678a5f2-3591-4633-ae98-9da8ddfc5690",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(tokens)\n",
    "print(word_freq.most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d843f6-572b-4142-aed5-63bb87e97993",
   "metadata": {},
   "source": [
    "To **exclude stopwords** (such as \"the\", \"of\", \"and\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6e632-b981-4dc9-af1f-eba64a91f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67100589-9b76-4c83-84d8-af5f2a092384",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [word for word in tokens if word not in STOPWORDS]\n",
    "word_freq = Counter(filtered_tokens)\n",
    "print(word_freq.most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ad0a3-ea0b-42c9-8324-8aed6045bdf6",
   "metadata": {},
   "source": [
    "### **Exercise 2:** \n",
    "\n",
    "Count the most frequent words in a Wikipedia article of your choice, excluding stopwords. See *Section 2.3 in Notebook 02* for how to extract and clean text from a Wikipedia article using `BeautifulSoup` and `requests`. After extracting the text, tokenize it into lowercase words, remove stopwords, and display the top 10 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b329430-842b-47ac-86b5-b69863a053c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94e4030d-8802-45d6-b228-cb22ac1c4842",
   "metadata": {},
   "source": [
    "For more advanced text processing, we can use the `spaCy` library. It handles punctuation, stopwords, and even finds the **base form** of each word (called lemma). Here, we use the headlines data again to explore how `spaCy` processes real-world text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96443eca-3c27-4939-842d-75f3604f56aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm # a small English language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f6c3f-944d-47b0-a8b6-73b580732627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read the text file with the headlines\n",
    "file_path = \"../Data/all_headlines.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]  # Clean & skip empty lines\n",
    "\n",
    "# Select 1000 random lines (headlines)\n",
    "import random\n",
    "random_lines = random.sample(lines, min(1000, len(lines)))\n",
    "print(random_lines[:10])\n",
    "\n",
    "# Join lines into a single string\n",
    "text = \" \".join(random_lines)\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text.lower())\n",
    "\n",
    "# Extract lemmas, removing stopwords and punctuation\n",
    "lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# Count the most frequent lemmas\n",
    "word_freq = Counter(lemmas)\n",
    "print(word_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4657ff79-17bb-47dd-a737-71d3c8721960",
   "metadata": {},
   "source": [
    "## 3.2 Word Cloud Visualization\n",
    "\n",
    "A word cloud is a fun and simple way to visualize word frequency: The more often a word appears, the bigger it shows up in the cloud. Let's built a word cloud from the headlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb42ab-6e0a-44b1-b64d-087efb4a182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400).generate(text)\n",
    "\n",
    "# Display it\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03525d77-2438-4a3a-af63-2702e5487db2",
   "metadata": {},
   "source": [
    "## 3.3 Sentiment Analysis with `TextBlob`\n",
    "\n",
    "`TextBlob` gives a quick and simple way to check the emotional tone of a sentence - whether it's positive, negative, or neutral. `TextBlob` analyzes the text and returns:\n",
    "\n",
    "- Polarity: -1 (negative) → +1 (positive)\n",
    "- Subjectivity: 0 (objective) → 1 (subjective)\n",
    "\n",
    "This is based on a built-in dictionary of words with assigned sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741528a-97e9-49fd-9c00-0842e00a6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb0bba-662d-4c29-b9f4-ec3d32322f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Python is amazing. But sometimes debugging makes me sad.\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889407ce-27bf-4adf-ba3f-9bfa08694579",
   "metadata": {},
   "source": [
    "### **Exercise 3:** \n",
    "\n",
    "Try sentences or paragraphs with or without clear emotional tone - negative or positive - and check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dffd79-4991-4d22-a245-80967517667a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c5f9590-a99d-427a-aeff-9116c86a86bb",
   "metadata": {},
   "source": [
    "## 4. Outlook: What's Next After the Basics?\n",
    "\n",
    "Once you’ve mastered data handling, and simple analysis with tools like `re` and `spaCy`, you’re well on your way to **more advanced text methods**. In fact, with qualitative data in Python, you very quickly enter the territory of **machine learning (ML)**.\n",
    "\n",
    "Why? Because qualitative data - everything that’s not numbers - must be turned into a structure that code can process. Programming is built around structured data and numeric operations, so we often need to **transform language into numbers**. \n",
    "\n",
    "**General-purpose libraries** like `scikit-learn` and `transformers` provide broad frameworks for many types of machine learning tasks, including but not limited to NLP. In addition, various **specialized libraries within NLP and qualitative research** exists, for example `whisper` for audio-to-text transcription or `BookNLP` for structural/narrative analysis of long texts (e.g., novels).\n",
    "\n",
    "This opens the door to **various powerful techniques and use-cases:**\n",
    "\n",
    "- *Named Entity Recognition (NER) & Entity-Level Structuring:* Detect and categorize names of people, places, organizations, etc.\n",
    "- *Topic Discovery & Interpretation:* Identify recurring themes across documents using unsupervised methods such as Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF).\n",
    "- *Audio-to-Text Transcription:* Convert spoken interviews or discussions into text.\n",
    "- *Text Classification:* Assign predefined labels (e.g., topic, urgency) using rule-based, classical ML (logistic regression, SVM), or neural approaches.\n",
    "- *Semantic Text Comparison & Embeddings:* Compare meaning across texts using vector-based representations.\n",
    "- *Keyword Extraction & Summarization:* Extract key phrases or generate concise summaries.\n",
    "- *Dimensionality Reduction & Text Clustering:* Explore or visualize hidden patterns in text embeddings.\n",
    "- *Stylometry & Authorship Analysis:* Analyze writing style to attribute or profile authorship.\n",
    "- *Narrative & Discourse Structure Analysis:* Analyze how information is organized.\n",
    "- *Network Analysis from Text:* Visualize relationships between entities or concepts, e.g. social/co-occurrence networks (e.g., speaker-topic links).\n",
    "- *Multilingual & Cross-Lingual NLP:* Analyze or compare texts in multiple languages.\n",
    "- *Long-Form Narrative Analysis:* Specialized tools for processing novels or long stories.\n",
    "- *Structured & Metadata-Aware Text Processing:* Parse and analyze structured documents like XML or JSON.\n",
    "- *Time-Based or Diachronic Text Analysis:* Track language or theme changes over time.\n",
    "- ...\n",
    "\n",
    "Note: When using large datasets together with advanced models (like transformer models), you'll often need more **computational resources**, such as a GPU or a computing cluster. For smaller datasets and simpler models (e.g., with scikit-learn), standard hardware usually is enough.\n",
    "\n",
    "**For more**, check out the [Python Humanities Textbook](python-textbook.pythonhumanities.com), which covers some of the mentioned techniques and use-cases from a humanities and qualitative research perspective. And check out Notebook 00 for more support. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4b909-9dc4-4a3b-a43e-25966bc039be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
